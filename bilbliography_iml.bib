
@inreference{noauthor_generative_2020,
	title = {Generative adversarial network},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Generative_adversarial_network&oldid=957926947},
	abstract = {A generative adversarial network ({GAN}) is a class of machine learning frameworks invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a {GAN} trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, {GANs} have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.},
	booktitle = {Wikipedia},
	urldate = {2020-05-24},
	date = {2020-05-21},
	langid = {english},
	note = {Page Version {ID}: 957926947},
	file = {Snapshot:/Users/IML/Zotero/storage/75HAIK2W/index.html:text/html}
}

@inreference{noauthor_convolutional_2020,
	title = {Convolutional neural network},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=957262810},
	abstract = {In deep learning, a convolutional neural network ({CNN}, or {ConvNet}) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks ({SIANN}), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.{CNNs} are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "fully-connectedness" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. {CNNs} take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, {CNNs} are on the lower extreme.
Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
{CNNs} use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.},
	booktitle = {Wikipedia},
	urldate = {2020-05-24},
	date = {2020-05-17},
	langid = {english},
	note = {Page Version {ID}: 957262810},
	file = {Snapshot:/Users/IML/Zotero/storage/WHGC82XX/index.html:text/html}
}

@inreference{noauthor_function_2019,
	title = {Function approximation},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Function_approximation&oldid=909261151},
	abstract = {In general, a function approximation problem asks us to select a function among a well-defined class that closely matches ("approximates") a target function in a task-specific way. The need for function approximations arises in many branches of applied mathematics, and computer science in particular.
One can distinguish two major classes of function approximation problems: 
First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).
Second, the target function, call it g, may be unknown; instead of an explicit formula, only a set of points of the form (x, g(x)) is provided.  Depending on the structure of the domain and codomain of g, several techniques for approximating g may be applicable.  For example, if g is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used.  If the codomain (range or target set) of g is a finite set, one is dealing with a classification problem instead.
To some extent, the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.},
	booktitle = {Wikipedia},
	urldate = {2020-05-24},
	date = {2019-08-04},
	langid = {english},
	note = {Page Version {ID}: 909261151},
	file = {Snapshot:/Users/IML/Zotero/storage/G3YEMMF8/index.html:text/html}
}

@inreference{noauthor_artificial_2020,
	title = {Artificial neural network},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=958242281},
	abstract = {Artificial neural networks ({ANN}) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.
An {ANN} is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.
In {ANN} implementations, the "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the {ANN} approach was to solve problems in the same way that a human brain would. But over time, attention moved to performing specific tasks, leading to deviations from biology. {ANNs} have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games, medical diagnosis, and even in activities that have traditionally been considered as reserved to humans, like painting.},
	booktitle = {Wikipedia},
	urldate = {2020-05-24},
	date = {2020-05-22},
	langid = {english},
	note = {Page Version {ID}: 958242281},
	file = {Snapshot:/Users/IML/Zotero/storage/VFRP6PYW/index.html:text/html}
}

@inreference{noauthor_q-learning_2020,
	title = {Q-learning},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Q-learning&oldid=956462482},
	abstract = {Q-learning is a model-free reinforcement learning algorithm to learn a policy telling an agent what action to take under what circumstances. It does not require a model (hence the connotation "model-free") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.
For any finite Markov decision process ({FMDP}), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given {FMDP}, given infinite exploration time and a partly-random policy. "Q" names the function that returns the reward used to provide the reinforcement and can be said to stand for the "quality" of an action taken in a given state.},
	booktitle = {Wikipedia},
	urldate = {2020-05-24},
	date = {2020-05-13},
	langid = {english},
	note = {Page Version {ID}: 956462482},
	file = {Snapshot:/Users/IML/Zotero/storage/WXIAFFKN/index.html:text/html}
}

@online{noauthor_reinforcement_2018,
	title = {Reinforcement learning},
	url = {https://www.geeksforgeeks.org/what-is-reinforcement-learning/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	titleaddon = {{GeeksforGeeks}},
	urldate = {2020-05-24},
	date = {2018-04-25},
	langid = {american},
	note = {Library Catalog: www.geeksforgeeks.org
Section: Advanced Computer Subject},
	file = {Snapshot:/Users/IML/Zotero/storage/S7LIDRYQ/what-is-reinforcement-learning.html:text/html}
}

@online{noauthor_convolutional_2017,
	title = {Convolutional Neural Networks in Python},
	url = {https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python},
	abstract = {In this tutorial, you’ll learn how to implement Convolutional Neural Networks ({CNNs}) in Python with Keras, and how to overcome overfitting with dropout.},
	titleaddon = {{DataCamp} Community},
	urldate = {2020-05-24},
	date = {2017-12-05},
	note = {Library Catalog: www.datacamp.com},
	file = {Snapshot:/Users/IML/Zotero/storage/DWTNYJLG/convolutional-neural-networks-python.html:text/html}
}

@online{noauthor_convolutional_2017-1,
	title = {Convolutional Neural Networks in Python},
	url = {https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python},
	abstract = {In this tutorial, you’ll learn how to implement Convolutional Neural Networks ({CNNs}) in Python with Keras, and how to overcome overfitting with dropout.},
	titleaddon = {{DataCamp} Community},
	urldate = {2020-05-24},
	date = {2017-12-05},
	note = {Library Catalog: www.datacamp.com},
	file = {Snapshot:/Users/IML/Zotero/storage/M6AYET8J/convolutional-neural-networks-python.html:text/html}
}

@online{noauthor_benchmark_nodate,
	title = {Benchmark dashboard},
	url = {http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
	urldate = {2020-05-24},
	file = {Benchmark dashboard:/Users/IML/Zotero/storage/TRIX5DLC/fashion-mnist.s3-website.eu-central-1.amazonaws.com.html:text/html}
}

@online{noauthor_demystifying_nodate,
	title = {Demystifying Deep Reinforcement Learning {\textbar} Computational Neuroscience Lab},
	url = {https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/},
	urldate = {2020-05-27},
	langid = {american},
	note = {Library Catalog: neuro.cs.ut.ee},
	file = {Snapshot:/Users/IML/Zotero/storage/7PIKAGFY/demystifying-deep-reinforcement-learning.html:text/html}
}